{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catalyst_disaster_tweet_bert_embeds.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEQYU-_Uwm-j"
      },
      "source": [
        "Example of fine-tuning Bert with Catalyst for https://www.kaggle.com/c/nlp-getting-started\n",
        "\n",
        "Some pieces of code are taken from https://github.com/Yorko/bert-finetuning-catalyst"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-5mOVMj2pVW"
      },
      "source": [
        "!pip install -U catalyst\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yp1XvVs3XH_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfA3w-Pr4V9X"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy\n",
        "from transformers import BertTokenizer, AutoConfig, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnzJCkQ-5LjM"
      },
      "source": [
        "SEED = 42\n",
        "BATCH_SIZE = 16\n",
        "ACCUM_STEPS = 4  # one optimization step for that many backward passes\n",
        "LEARNING_RATE = 3e-5   # learning rate is typically ~1e-5 for transformers\n",
        "EPOCHS = 4 # smth around 2-6 epochs is typically fine when finetuning transformers\n",
        "MAX_LEN = 256 # TODO maybe better in flow\n",
        "BERT_SHORTCUT_NAME = 'bert-base-multilingual-cased'\n",
        "\n",
        "DATA_PATH =  \"/content/drive/My Drive/bell/fine_tune_bench/disaster_tweet/\"\n",
        "LOG_DIR = './logdir/'\n",
        "TRAIN_DATA = \"train.csv\"\n",
        "VALID_DATA = \"valid.csv\"\n",
        "TEST_DATA = \"test.csv\"\n",
        "\n",
        "SENTENCE_LABEL = 'text'\n",
        "TARGET_LABEL = 'target'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlmHg379D6KJ"
      },
      "source": [
        "class BertClassificationDataset(Dataset):\n",
        "  def __init__(self, \n",
        "               texts, \n",
        "               labels = None, \n",
        "               label2class = None,\n",
        "               max_len = 512, \n",
        "               bert_model_name = BERT_SHORTCUT_NAME,\n",
        "      ):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.label2class = label2class\n",
        "    self.max_len = max_len\n",
        "    if self.label2class is None and labels is not None:\n",
        "      # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
        "      # no easily handle unknown target values\n",
        "      self.label2class = dict(zip(sorted(set(labels)), range(len(set(labels)))))    \n",
        "    self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "    # suppresses tokenizer warnings\n",
        "    # logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # encoding the text\n",
        "        x = self.texts[index]\n",
        "\n",
        "        # a dictionary with `input_ids` and `attention_mask` as keys\n",
        "        output_dict = self.tokenizer.encode_plus(\n",
        "            x,\n",
        "            add_special_tokens=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        # for Catalyst, there needs to be a key called features\n",
        "        output_dict[\"features\"] = output_dict[\"input_ids\"].squeeze(0)\n",
        "        del output_dict[\"input_ids\"]\n",
        "\n",
        "        # encoding target\n",
        "        if self.labels is not None:\n",
        "            y = self.labels[index]\n",
        "            y_encoded = torch.Tensor([self.label2class.get(y, -1)]).long().squeeze(0)\n",
        "            output_dict[\"targets\"] = y_encoded\n",
        "\n",
        "        return output_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeoxpUSotqdk"
      },
      "source": [
        "# TODO compare classifcation with pooling and CLS token\n",
        "class BertClassifierModel(nn.Module):\n",
        "  def __init__(self, num_classes, bert_model_name=BERT_SHORTCUT_NAME, freeze_bert = False, dropout = 0.3):\n",
        "    super().__init__()\n",
        "    \n",
        "    config = AutoConfig.from_pretrained(bert_model_name, num_labels=num_classes)\n",
        "    self.model = AutoModel.from_pretrained(bert_model_name, config=config)\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "      for p in self.model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
        "    self.dropout = nn.Dropout(dropout)    \n",
        "  \n",
        "  def forward(self, features, attention_mask=None, head_mask=None):\n",
        "    assert attention_mask is not None, \"attention mask is none\"\n",
        "\n",
        "    # taking BERTModel output\n",
        "    # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
        "    bert_output = self.model(\n",
        "            input_ids=features, attention_mask=attention_mask, head_mask=head_mask\n",
        "        )\n",
        "    # we only need the hidden state here and don't need\n",
        "    # transformer output, so index 0\n",
        "    seq_output = bert_output[0]  # (bs, seq_len, dim)\n",
        "    # mean pooling, i.e. getting average representation of all tokens\n",
        "    #pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n",
        "    #pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "    #scores = self.classifier(pooled_output)  # (bs, num_classes)\n",
        "    cls_rep = seq_output[:, 0]\n",
        "    cls_rep = self.dropout(cls_rep)  # (bs, dim)\n",
        "    scores = self.classifier(cls_rep)  # (bs, num_classes)\n",
        "    return scores"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdVuhVFOKUDV"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(DATA_PATH, TRAIN_DATA))\n",
        "valid_df = pd.read_csv(os.path.join(DATA_PATH, VALID_DATA))\n",
        "test_df = pd.read_csv(os.path.join(DATA_PATH, TEST_DATA))\n",
        "\n",
        "train_dataset = BertClassificationDataset(\n",
        "        texts=train_df[SENTENCE_LABEL].values.tolist(),\n",
        "        labels=train_df[TARGET_LABEL].values,\n",
        "        max_len=MAX_LEN,\n",
        "    )\n",
        "\n",
        "valid_dataset = BertClassificationDataset(\n",
        "        texts=valid_df[SENTENCE_LABEL].values.tolist(),\n",
        "        labels=valid_df[TARGET_LABEL].values,\n",
        "        max_len=MAX_LEN,\n",
        "    )\n",
        "\n",
        "test_dataset = BertClassificationDataset(\n",
        "        texts=test_df[SENTENCE_LABEL].values.tolist(),\n",
        "        max_len=MAX_LEN,\n",
        "    )\n",
        "\n",
        "train_val_loaders = {\n",
        "        \"train\": DataLoader(\n",
        "            dataset=train_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "        ),\n",
        "        \"valid\": DataLoader(\n",
        "            dataset=valid_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "        ),\n",
        "    }\n",
        "\n",
        "test_loaders = {\n",
        "        \"test\": DataLoader(\n",
        "            dataset=test_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "        )\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waHImMUzpZes"
      },
      "source": [
        "from catalyst.dl import SupervisedRunner\n",
        "from catalyst.dl.callbacks import (\n",
        "    AccuracyCallback,\n",
        "    CheckpointCallback,\n",
        "    InferCallback,\n",
        "    OptimizerCallback,\n",
        ")\n",
        "from catalyst.utils import prepare_cudnn, set_global_seed\n",
        "\n",
        "\n",
        "num_classes = len(set(train_df[TARGET_LABEL].values))\n",
        "model = BertClassifierModel(num_classes)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=float(LEARNING_RATE)\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "set_global_seed(SEED)\n",
        "prepare_cudnn(deterministic=True)\n",
        "\n",
        "runner = SupervisedRunner(input_key=(\"features\", \"attention_mask\"))\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders=train_val_loaders,\n",
        "    callbacks=[\n",
        "        AccuracyCallback(num_classes=int(num_classes)),\n",
        "        OptimizerCallback(accumulation_steps=int(ACCUM_STEPS)),\n",
        "    ],\n",
        "    logdir=LOG_DIR,\n",
        "    num_epochs=EPOCHS,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# and running inference\n",
        "torch.cuda.empty_cache()\n",
        "runner.infer(\n",
        "    model=model,\n",
        "    loaders=test_loaders,\n",
        "    callbacks=[\n",
        "        CheckpointCallback(\n",
        "            resume=f\"{LOG_DIR}/checkpoints/best.pth\"\n",
        "        ),\n",
        "        InferCallback(),\n",
        "    ],\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUyiTV6ZHHpE"
      },
      "source": [
        "# inference\n",
        "# lastly, saving predicted scores for the test set\n",
        "predicted_scores = runner.callbacks[0].predictions[\"logits\"]\n",
        "predicted_labels = predicted_scores.argmax(-1)\n",
        "subm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n",
        "subm[TARGET_LABEL] = predicted_labels\n",
        "subm.to_csv(os.path.join(DATA_PATH, 'submission_catalyst.csv'), index = None)"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}