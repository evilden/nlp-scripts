{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catalyst_disaster_tweet_bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEQYU-_Uwm-j"
      },
      "source": [
        "Example of fine-tuning Bert with Catalyst for https://www.kaggle.com/c/nlp-getting-started\n",
        "\n",
        "Some pieces of code are taken from https://github.com/Yorko/bert-finetuning-catalyst"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-5mOVMj2pVW",
        "outputId": "d1d02192-2bd2-42fd-cee2-6c5564075ef0"
      },
      "source": [
        "!pip install -U catalyst\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: catalyst in /usr/local/lib/python3.6/dist-packages (20.12)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.33.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboardX>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from catalyst) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=2.1.0->catalyst) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=2.1.0->catalyst) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->catalyst) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->catalyst) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->catalyst) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->catalyst) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX>=2.1.0->catalyst) (53.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yp1XvVs3XH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721dcd75-cc22-4a94-8538-1c9aca797532"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfA3w-Pr4V9X"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy\n",
        "from transformers import BertTokenizer, AutoConfig, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnzJCkQ-5LjM"
      },
      "source": [
        "SEED = 42\n",
        "BATCH_SIZE = 16\n",
        "ACCUM_STEPS = 4  # one optimization step for that many backward passes\n",
        "LEARNING_RATE = 3e-5   # learning rate is typically ~1e-5 for transformers\n",
        "EPOCHS = 4 # smth around 2-6 epochs is typically fine when finetuning transformers\n",
        "MAX_LEN = 256 # TODO maybe better in flow\n",
        "BERT_SHORTCUT_NAME = 'bert-base-multilingual-cased'\n",
        "\n",
        "DATA_PATH =  \"/content/drive/My Drive/bell/fine_tune_bench/disaster_tweet/\"\n",
        "LOG_DIR = './logdir/'\n",
        "TRAIN_DATA = \"train.csv\"\n",
        "VALID_DATA = \"valid.csv\"\n",
        "TEST_DATA = \"test.csv\"\n",
        "\n",
        "SENTENCE_LABEL = 'text'\n",
        "TARGET_LABEL = 'target'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpoKQcFG5WkV"
      },
      "source": [
        "# TODO \n",
        "# - split input train for train and valid if valid doesn't exist\n",
        "# - slanted traingle learning rate or similar"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlmHg379D6KJ"
      },
      "source": [
        "class BertClassificationDataset(Dataset):\n",
        "  def __init__(self, \n",
        "               texts, \n",
        "               labels = None, \n",
        "               label2class = None,\n",
        "               max_len = 512, \n",
        "               bert_model_name = BERT_SHORTCUT_NAME,\n",
        "      ):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.label2class = label2class\n",
        "    self.max_len = max_len\n",
        "    if self.label2class is None and labels is not None:\n",
        "      # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
        "      # no easily handle unknown target values\n",
        "      self.label2class = dict(zip(sorted(set(labels)), range(len(set(labels)))))    \n",
        "    self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "    # suppresses tokenizer warnings\n",
        "    # logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        # encoding the text\n",
        "        x = self.texts[index]\n",
        "\n",
        "        # a dictionary with `input_ids` and `attention_mask` as keys\n",
        "        output_dict = self.tokenizer.encode_plus(\n",
        "            x,\n",
        "            add_special_tokens=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        # for Catalyst, there needs to be a key called features\n",
        "        output_dict[\"features\"] = output_dict[\"input_ids\"].squeeze(0)\n",
        "        del output_dict[\"input_ids\"]\n",
        "\n",
        "        # encoding target\n",
        "        if self.labels is not None:\n",
        "            y = self.labels[index]\n",
        "            y_encoded = torch.Tensor([self.label2class.get(y, -1)]).long().squeeze(0)\n",
        "            output_dict[\"targets\"] = y_encoded\n",
        "\n",
        "        return output_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdVuhVFOKUDV"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(DATA_PATH, TRAIN_DATA))\n",
        "valid_df = pd.read_csv(os.path.join(DATA_PATH, VALID_DATA))\n",
        "test_df = pd.read_csv(os.path.join(DATA_PATH, TEST_DATA))\n",
        "\n",
        "train_dataset = BertClassificationDataset(\n",
        "        texts=train_df[SENTENCE_LABEL].values.tolist(),\n",
        "        labels=train_df[TARGET_LABEL].values,\n",
        "        max_len=MAX_LEN,\n",
        "    )\n",
        "\n",
        "valid_dataset = BertClassificationDataset(\n",
        "        texts=valid_df[SENTENCE_LABEL].values.tolist(),\n",
        "        labels=valid_df[TARGET_LABEL].values,\n",
        "        max_len=MAX_LEN,\n",
        "    )\n",
        "\n",
        "test_dataset = BertClassificationDataset(\n",
        "        texts=test_df[SENTENCE_LABEL].values.tolist(),\n",
        "        max_len=MAX_LEN,\n",
        "    )\n",
        "\n",
        "train_val_loaders = {\n",
        "        \"train\": DataLoader(\n",
        "            dataset=train_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "        ),\n",
        "        \"valid\": DataLoader(\n",
        "            dataset=valid_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "        ),\n",
        "    }\n",
        "\n",
        "test_loaders = {\n",
        "        \"test\": DataLoader(\n",
        "            dataset=test_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "        )\n",
        "    }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeoxpUSotqdk"
      },
      "source": [
        "# TODO compare classifcation with pooling and CLS token\n",
        "class BertClassifierModel(nn.Module):\n",
        "  def __init__(self, num_classes, bert_model_name=BERT_SHORTCUT_NAME, freeze_bert = False, dropout = 0.3):\n",
        "    super().__init__()\n",
        "    \n",
        "    config = AutoConfig.from_pretrained(bert_model_name, num_labels=num_classes)\n",
        "    self.model = AutoModel.from_pretrained(bert_model_name, config=config)\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "      for p in self.model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
        "    self.dropout = nn.Dropout(dropout)    \n",
        "  \n",
        "  def forward(self, features, attention_mask=None, head_mask=None):\n",
        "    assert attention_mask is not None, \"attention mask is none\"\n",
        "\n",
        "    # taking BERTModel output\n",
        "    # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
        "    bert_output = self.model(\n",
        "            input_ids=features, attention_mask=attention_mask, head_mask=head_mask\n",
        "        )\n",
        "    # we only need the hidden state here and don't need\n",
        "    # transformer output, so index 0\n",
        "    seq_output = bert_output[0]  # (bs, seq_len, dim)\n",
        "    # mean pooling, i.e. getting average representation of all tokens\n",
        "    pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n",
        "    pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "    scores = self.classifier(pooled_output)  # (bs, num_classes)\n",
        "\n",
        "    return scores"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHEzflcRZaQg"
      },
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified version of the same class by HuggingFace.\n",
        "    See transformers/modeling_distilbert.py in the transformers repository.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, pretrained_model_name: str, num_classes: int = None, dropout: float = 0.3\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pretrained_model_name (str): HuggingFace model name.\n",
        "                See transformers/modeling_auto.py\n",
        "            num_classes (int): the number of class labels\n",
        "                in the classification task\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            pretrained_model_name, num_labels=num_classes\n",
        "        )\n",
        "\n",
        "        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, features, attention_mask=None, head_mask=None):\n",
        "\n",
        "        assert attention_mask is not None, \"attention mask is none\"\n",
        "\n",
        "        # taking BERTModel output\n",
        "        # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
        "        bert_output = self.model(\n",
        "            input_ids=features, attention_mask=attention_mask, head_mask=head_mask\n",
        "        )\n",
        "        # we only need the hidden state here and don't need\n",
        "        # transformer output, so index 0\n",
        "        seq_output = bert_output[0]  # (bs, seq_len, dim)\n",
        "        \n",
        "        # mean pooling, i.e. getting average representation of all tokens\n",
        "        #pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n",
        "        #pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        #scores = self.classifier(pooled_output)  # (bs, num_classes)\n",
        "        \n",
        "        cls_rep = seq_output[:, 0]\n",
        "        cls_rep = self.dropout(cls_rep)  # (bs, dim)\n",
        "        scores = self.classifier(cls_rep)  # (bs, num_classes)\n",
        "\n",
        "        return scores"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYq0QQ4omauB"
      },
      "source": [
        "# from transformers import BertForSequenceClassification\n",
        "\n",
        "# num_classes = len(set(train_df[TARGET_LABEL].values))\n",
        "# config = AutoConfig.from_pretrained(BERT_SHORTCUT_NAME, num_labels=num_classes)\n",
        "# model = BertForSequenceClassification.from_pretrained(BERT_SHORTCUT_NAME, config=config)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkDjcP0-cnXO"
      },
      "source": [
        "# class BertClassifierModel(nn.Module):\n",
        "  # def __init__(self, num_classes, bert_model_name=BERT_SHORTCUT_NAME, freeze_bert = False, dropout = 0.3):\n",
        "num_classes = len(set(train_df[TARGET_LABEL].values))\n",
        "model = BertClassifierModel(num_classes)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waHImMUzpZes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e4c20a-86d7-4e6a-ada1-e4ad394dd3ea"
      },
      "source": [
        "from catalyst.dl import SupervisedRunner\n",
        "from catalyst.dl.callbacks import (\n",
        "    AccuracyCallback,\n",
        "    CheckpointCallback,\n",
        "    InferCallback,\n",
        "    OptimizerCallback,\n",
        ")\n",
        "from catalyst.utils import prepare_cudnn, set_global_seed\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=float(LEARNING_RATE)\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "\n",
        "set_global_seed(SEED)\n",
        "prepare_cudnn(deterministic=True)\n",
        "\n",
        "runner = SupervisedRunner(input_key=(\"features\", \"attention_mask\"))\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders=train_val_loaders,\n",
        "    callbacks=[\n",
        "        AccuracyCallback(num_classes=int(num_classes)),\n",
        "        OptimizerCallback(accumulation_steps=int(ACCUM_STEPS)),\n",
        "    ],\n",
        "    logdir=LOG_DIR,\n",
        "    num_epochs=EPOCHS,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# and running inference\n",
        "torch.cuda.empty_cache()\n",
        "runner.infer(\n",
        "    model=model,\n",
        "    loaders=test_loaders,\n",
        "    callbacks=[\n",
        "        CheckpointCallback(\n",
        "            resume=f\"{LOG_DIR}/checkpoints/best.pth\"\n",
        "        ),\n",
        "        InferCallback(),\n",
        "    ],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# lastly, saving predicted scores for the test set\n",
        "predicted_scores = runner.callbacks[0].predictions[\"logits\"]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/4 * Epoch (train): 100% 381/381 [02:36<00:00,  2.43it/s, accuracy01=0.900, loss=0.299]\n",
            "1/4 * Epoch (valid): 100% 96/96 [00:14<00:00,  6.78it/s, accuracy01=0.500, loss=0.866]\n",
            "[2021-02-07 23:37:15,613] \n",
            "1/4 * Epoch 1 (_base): lr=3.000e-05 | momentum=0.9000\n",
            "1/4 * Epoch 1 (train): accuracy01=0.7901 | loss=0.4641\n",
            "1/4 * Epoch 1 (valid): accuracy01=0.8338 | loss=0.3913\n",
            "2/4 * Epoch (train): 100% 381/381 [02:36<00:00,  2.44it/s, accuracy01=0.800, loss=0.365]\n",
            "2/4 * Epoch (valid): 100% 96/96 [00:14<00:00,  6.76it/s, accuracy01=0.500, loss=1.067]\n",
            "[2021-02-07 23:41:32,901] \n",
            "2/4 * Epoch 2 (_base): lr=3.000e-05 | momentum=0.9000\n",
            "2/4 * Epoch 2 (train): accuracy01=0.8580 | loss=0.3449\n",
            "2/4 * Epoch 2 (valid): accuracy01=0.8325 | loss=0.3981\n",
            "3/4 * Epoch (train): 100% 381/381 [02:36<00:00,  2.44it/s, accuracy01=0.900, loss=0.184]\n",
            "3/4 * Epoch (valid): 100% 96/96 [00:14<00:00,  6.76it/s, accuracy01=0.500, loss=0.938]\n",
            "[2021-02-07 23:45:16,367] \n",
            "3/4 * Epoch 3 (_base): lr=3.000e-05 | momentum=0.9000\n",
            "3/4 * Epoch 3 (train): accuracy01=0.8905 | loss=0.2728\n",
            "3/4 * Epoch 3 (valid): accuracy01=0.8279 | loss=0.4073\n",
            "4/4 * Epoch (train): 100% 381/381 [02:36<00:00,  2.43it/s, accuracy01=1.000, loss=0.012]\n",
            "4/4 * Epoch (valid): 100% 96/96 [00:14<00:00,  6.76it/s, accuracy01=0.500, loss=1.924]\n",
            "[2021-02-07 23:49:00,006] \n",
            "4/4 * Epoch 4 (_base): lr=3.000e-05 | momentum=0.9000\n",
            "4/4 * Epoch 4 (train): accuracy01=0.9279 | loss=0.1922\n",
            "4/4 * Epoch 4 (valid): accuracy01=0.8351 | loss=0.5266\n",
            "Top best models:\n",
            "logdir/checkpoints/train.1.pth\t0.3913\n",
            "=> Loading checkpoint ./logdir//checkpoints/best.pth\n",
            "loaded state checkpoint ./logdir//checkpoints/best.pth (global epoch 1, epoch 1, stage train)\n",
            "1/1 * Epoch (test): 100% 204/204 [00:30<00:00,  6.77it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUyiTV6ZHHpE"
      },
      "source": [
        "predicted_labels = predicted_scores.argmax(-1)\n",
        "subm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n",
        "subm[TARGET_LABEL] = predicted_labels\n",
        "subm.to_csv(os.path.join(DATA_PATH, 'submission_catalyst.csv'), index = None)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S84caf6i6_Jo"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}